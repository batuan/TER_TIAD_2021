{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from collections import defaultdict  # For word frequency\n",
    "from gensim import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_en_fr = ['ES-GL', 'ES-EO', 'CA-FR', 'EN-EU', 'ES-OC', 'EO-CA', 'FR-ES', 'ES-FR', 'OC-ES', 'EN-ES', 'EN-GL', 'OC-CA', 'EO-FR', 'EN-CA', 'ES-CA', 'GL-EN', 'OC-FR', 'EO-ES', 'CA-ES', 'EN-EO', 'EU-EN', 'CA-EO', 'EO-EN', 'FR-EO', 'ES-EN', 'FR-CA', 'CA-OC', 'CA-EN', 'EU-ES', 'ES-EU', 'FR-OC', 'GL-ES']\n",
    "def check_satisfy_en_fr(path):\n",
    "    for x in list_en_fr:\n",
    "        if x in path:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = os.listdir('../Small')\n",
    "lists = [x for x in lists if x.endswith('.csv')]\n",
    "lists = [x for x in lists if check_satisfy_en_fr(x)]\n",
    "# mypd = [pd.read_csv('./Small/' + x) for x in lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pd.read_csv('../EvaluationDic/En-Frv2.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_source = dictionary['source'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = []\n",
    "for x in lists[0:1]:\n",
    "    df = pd.read_csv('../Small/' + x)\n",
    "    for k in df['source'].tolist():\n",
    "        array.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coloni',\n",
       " 'Atienza',\n",
       " 'Viñas',\n",
       " 'réfrigérateur',\n",
       " 'Marqués',\n",
       " 'Cavassa',\n",
       " 'Boquillon',\n",
       " 'Gagnière',\n",
       " 'Llinas',\n",
       " 'Wundt',\n",
       " 'Soriero',\n",
       " 'holocène',\n",
       " 'féminin',\n",
       " 'con spirito',\n",
       " 'barème']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list(set(array) - set(dic_source)))[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransSetEN-CA_small.csv\n",
      "TransSetEN-ES_small.csv\n",
      "TransSetEN-GL_small.csv\n",
      "TransSetEO-CA_small.csv\n",
      "TransSetEO-EN_small.csv\n",
      "TransSetEO-ES_small.csv\n",
      "TransSetEO-FR_small.csv\n",
      "TransSetES-CA_small.csv\n",
      "TransSetES-GL_small.csv\n",
      "TransSetEU-EN_small.csv\n",
      "TransSetEU-ES_small.csv\n",
      "TransSetFR-CA_small.csv\n",
      "TransSetFR-ES_small.csv\n",
      "TransSetOC-CA_small.csv\n",
      "TransSetOC-ES_small.csv\n",
      "TransSetOC-FR_small.csv\n"
     ]
    }
   ],
   "source": [
    "list.sort(lists)\n",
    "for x in lists:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_target(x):\n",
    "    source = x.split('Set')[1].split('-')[0]\n",
    "    target = x.split('Set')[1].split('-')[1].split('_')[0]\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_sentense(file):\n",
    "    df = pd.read_csv('../Small/' + file)\n",
    "    source, target = get_source_target(file) \n",
    "    df['source'] = df['source'].apply(lambda x: \"_\".join(str(x).split()))\n",
    "    df['target'] = df['target'].apply(lambda x: \"_\".join(str(x).split()))\n",
    "    df['source_join'] = df['source']+'_'+ source + '_' + df['POS']\n",
    "    df['target_join'] = df['target']+'_'+ target + '_' + df['POS']\n",
    "    df['translate1'] = df['source_join'] + ' ' + df['target_join'] + ' ' + df['source_join'] +' ' + df['target_join']\n",
    "    # df['translate2'] = df['target'] +'_'+ df['POS'] + ' ' + df['source'] + '_' + df['POS'] \n",
    "    # df['translate2'] = df['translate2'].astype(str)\n",
    "#     array = [x.split() for x in df['translate1'].tolist()]\n",
    "    array = [x for x in df['translate1'].tolist()]\n",
    "    return array#\" \".join(df['translate1']) #+ \" \".join(df['translate2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [df_to_sentense(x) for x in lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write sent to file:\n",
    "with open('../data_train/data_train.txt', 'w') as f:\n",
    "    for a in sent:\n",
    "        for item in a:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [df_to_sentense(x) for x in lists]\n",
    "#sent = [row.split() for row in sent]\n",
    "\n",
    "#create corpus by joint all line in sent\n",
    "corpus = []\n",
    "for se in sent:\n",
    "    for line in se:\n",
    "        corpus.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=corpus, size=300, iter=300, window=1, min_count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203004, 300)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/first_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doako_adjective', 0.992991030216217),\n",
       " ('gratuíto_adjective', 0.9928751587867737),\n",
       " ('gratuito_adjective', 0.9925113916397095),\n",
       " ('aske_adjective', 0.9909542202949524),\n",
       " ('senpaga_adjective', 0.9906609058380127),\n",
       " ('lliure_adjective', 0.9890164732933044),\n",
       " ('gratuit_adjective', 0.988632082939148),\n",
       " ('liure_adjective', 0.9878025054931641),\n",
       " ('lineare_nedependa_adjective', 0.9868024587631226),\n",
       " ('gratuït_adjective', 0.9865648746490479)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('free_adjective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03385356,  0.13950163],\n",
       "       [ 0.23350666, -0.04644945]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[['crumb_noun', 'free_adjective']][0:2,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03385356,  0.13950163], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['crumb_noun'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict_fr = ['TransSetFR-CA_small.csv', 'TransSetFR-ES_small.csv']\n",
    "df_fr1 = pd.read_csv('./Small/' + path_dict_fr[0])\n",
    "df_fr2 = pd.read_csv('./Small/' + path_dict_fr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (df_fr1['source'].astype(str) + '_' + df_fr1['POS'].astype(str)).to_list()\n",
    "b = (df_fr2['source'].astype(str) + '_' + df_fr2['POS'].astype(str)).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-165-daf035289aa7>:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  france_vectors = model[france_dict]\n"
     ]
    }
   ],
   "source": [
    "france_dict = list(set(a) & set(b))\n",
    "france_dict = list(set(france_dict) & set(vocabulary))\n",
    "france_vectors = model[france_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_word(list_df):\n",
    "    words = []\n",
    "    for path in list_df:\n",
    "        df = pd.read_csv('./Small/' + path)\n",
    "        words += (df['source'].astype(str) + '_' + df['POS'].astype(str)).to_list()\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict_en = ['TransSetEN-CA_small.csv', 'TransSetEN-ES_small.csv', 'TransSetEN-GL_small.csv']\n",
    "words_en = to_word(path_dict_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-166-35fcd3405bfb>:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  english_vectors = model[words_en]\n"
     ]
    }
   ],
   "source": [
    "words_en = list(set(words_en) & set(vocabulary))\n",
    "english_vectors = model[words_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41145, 300)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(word):\n",
    "    return word.split('_')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_translate(word_vec, matrix_vec, dic_trans, pos):\n",
    "    assert len(matrix_vec) == len(dic_trans)\n",
    "    cosine_similary = model.wv.cosine_similarities(word_vec, matrix_vec)\n",
    "    sort_cosine = np.argsort(-cosine_similary)\n",
    "    for index in sort_cosine:\n",
    "        trans_word = dic_trans[index]\n",
    "        if(get_pos(trans_word)==pos):\n",
    "            return trans_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-259-497a10250b79>:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  word_vec = model[word]\n"
     ]
    }
   ],
   "source": [
    "dict_en_fr = []\n",
    "for word in words_en:\n",
    "    pos = get_pos(word)\n",
    "    word_vec = model[word]\n",
    "    trans_word = find_translate(word_vec, france_vectors, france_dict, pos)\n",
    "    if trans_word != None:\n",
    "        trans_word = to_word(trans_word)\n",
    "    dict_en_fr.append(to_word(word)+'\\t'+str(trans_word)+'\\t'+str(pos)+'\\t1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_word(word):\n",
    "    return ' '.join(word.split('_')[0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41145"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_en_fr.insert(0, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./BuildDictionary/TIADbaseline_trans_en-fr.tsv', 'w') as f:\n",
    "    for item in dict_en_fr:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
